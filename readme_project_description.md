# Humans

## Introduction
'Humans', is a project consisting of a series studies on semantics and meaning. There are two major components of the topic:
1. The structure of the word(linguistic) semantics
2. The relationship and interactions between world semantics and word semantics

Word semantics and world semantics are the two paramount constituents of the study of meaning. While the former concerns 
more about the meanings embedded and represented within language (a formal system), latter is more about the reality
of the 'physical' world (and mental representation in other modules, e.g. visual representation) that gives rise to the 
word semantics. The relation between these two components of meaning can be summarized as:
1. World semantics gives rise to word semantics, and word semantics is a 'formal' (here 'formal' broadly refers something
with a form, but not the canonical sense of 'rule based') representation, and also an approximate 'homomorphism' with 
some specific concentration.

2. Word semantics, while founded on World semantics, does not always adhere to it. As it is embedded in language, word 
semantics can be combined, composed recursively, creating infinite meanings that does not, or may not exists in the realistic
world.

In the project, we investigate both components of meaning, and speak to both relations (between world and word semantics)
mentioned above. 

Corresponding to the first relation, 'Humans' project collect all data from a simulated world, which an artificial language
built on. More details are provided in specific sessions, and here is a brief introduction. We create a world, consisting 
of creatures (humans, animals, plants) and 'physical' rules(law of the world), and generate events as the world runs 
automatically under some designed algorithm. Such a simulated running world equipped with physical rules provides a 
'world semantics'. A pseudo language is generated as the description of the world events, with a preposed syntax. As a
description of the simulated world, the language should be able to capture the preposed world rules, and featuring what
actually happened in the world.

Speaking to the second relation, the artificially generated language, given its characteristic of being a language, 
should not only serve as mere record of the world events, but also has the potential to generate new phrases, sentences
and concepts. Moreover, these new linguistic creations (generated by the language, not what actually happened in the world)
should not only follow the syntax, but also 'make sense'. That is, the language, created as a description of the what 
happened in the world, should be able to make analogies, and infer what may be possible, less possible, and impossible,
in other words, creating 'new meanings'.

And whether or not the 'new meaning' can be generated and in what 'form' is such a meaning? It depends on how we model the 
word semantics, which is one of the main themes of project Humans: how do different semantic models differ in terms of their
representational power and characteristic? It might be affected by the initial world semantic, by the form of the input 
(linguistic/non linguistic), by what and how to encode the information, what data structures is used to represent the concepts 
(spacial, in terms of vector space/graphical, like semantic network/connectionism like SRN), and factors are the aspects 
when it is to talk about semantic models.

By creating a world and deriving a language from the world, we are explicitly binding the world semantics and word 
semantics, thus making the relationship between the two controlled, and as a result, controlling many factors mentioned
above. And the goal of the study is, given the artificial corpus, deriving from a world, with the embedding world semantics
controlled, which, or what type of semantic models can:
1. **Capture the world semantics**
2. Basing on the model semantic representation, **making 'reasonable' inference** which provide possibility beyond the physical
world reality (and this is the critical representational power that we are examining here).

Notice that this project goal seems to coincide with the second relation between world semantic and word semantic, however
there is a minor distinction. In the project goal, since we have controlled factors including the input, the underlying
world semantics, the resultant representational difference can only be attributed to the representational characteristic
of the semantic model investigated. 

In the following sections, there are more detailed information about the project, which is organized as:
1. **Project goals in detail**: the recent project goals, parsed into steps.
2. **World**: Versions of simulated World corresponding to different steps of the project goals.
3. **Corpus**: How the corpora correspond to the simulated world, and the theoretical and empirical validity of the 
corpora as the input for the semantic models
4. **Semantic models**: Different semantic models and representations investigated in the series of study. 


## Project goals in detail

The current goal of the project is to figure out a semantic representational model accounting for the compositionality
of language and concept. Here we don't give a rigorous definition of the term 'compositionality', it roughly refers to the
notion that concepts functioning on each other forming new concepts, e.g. 'apple' and 'red' compose 'red apple',
'chase' and 'dog' form 'chase dog'. The goal of the project is to find semantic models, which 
1. Provide a compatible semantic representation for both the lexical items (words), and the composed structures 
(phrase, sentence).
2. Provide a semantic representation which articulate the relations among linguistic structures of all levels 
(word, phrase, sentence, morpheme?)

For example, the model should be able to:
1. represent the meaning of 'dog', 'chase', 'cat', 'cheat','mouse'
2. represent the meaning of composed structure 'chase cat', 'chase mouse', 'cheat dog', 'dog chase cat'
3. represent the semantic relations between the (meanings of) 'cat' and 'dog', 'chase cat' and 'chase dog', 'chase' and
'cheat', 'chase cat' and 'cheat cat', 'cat cheat dog' and 'cat chase mouse', 'dog chase cat',
and 'cat chase mouse'. The semantic relations mentioned can refer to 'similarity' or 'semantic'
relatedness in more general sense. For example, for 'cat' and 'dog', both relatedness and similarity
can be measured, and this is the same for 'chase cat' and 'chase mouse', while it makes more 
sense to measure the general relatedness between 'dog chase cat' and 'chase mouse'.

Such a goal concerning the representation of compositionality can be parsed into two steps:
1. first order dependency(D1): combinatorial properties of the semantics of concepts 
2. second order dependency(D2): representation of real composed concepts.

We spell out D1 and D2 in the example below:

### D1: Combinatorial property of the semantics of concepts

D1 concerns about the combinatorial properties of words. For example, we may say 'drink coffee'
and 'eat an apple', but not the other way round. Here, while 'drink an apple' and 'eat coffee' make little sense, in other words,
semantically bad, they are grammatically plausible, 'drink eat' on the other hand is syntactically bad. Throughout the discussion,
we only talk about linguistic structures with correct syntax while varies in semantic soundness.

Within the scope of grammatical structures, the semantic soundness of the items may not be a binary 'yes or no' standard, but a spectrum.
For example, while it is weird to say 'eat coffee', it is even worse to say 'eat a table', and 'eat the song' and 'eat concentration' 
sound extremely weird, on the other hand, 'eat a feast' would be better than 'eat coffee' while not as good as 'eat an apple'.

This shows some words are more semantically plausible to be coupled than other pairs, which resulting in the generic combinatorial
properties of lexical items: while the verb 'drink' takes 'drinks' as its canonical argument, 'eat' takes solid foods.
What follows from this, is a generic classification (grouping, distancing) of words(usually within one grammatical category) 
by its environment (the words it couples) with. This general philosophy of carving out the meaning of a word by its context
is followed by a wide range of semantic models, including feature norms, and spatial models encoding contextual information(LSA, topic model). 

And this combinatorial property of words: how likely a word is to be coupled with another word (under the condition of 
being grammatical) is what we refer as D1 here: the default semantic dependency, 'eat' goes with food, and 'drink' goes with
liquid. Going back to the general goal discussed in the introduction, a semantic representation should be able to:
1. represent D1 embedded in the input (world semantics)
2. form inference of other D1 relation, which is not observed in the input

For example, suppose in the corpus, we observed lots of 'eat apple' and 'eat banana', and another noun 'lingou' which has
never been coupled with 'eat'in the corpus. The semantic model should first in the representation account for the 'fact' that
'eat' can go with 'apple' and banana, while infer whether 'eat lingou' make sense, basing on how similar is 'lingou' to
'apple' and 'banana'. If in the corpus, lingou always appear in the exact same position as 'apple' and 
'banana' wihtin a sentence, and it couples with lots of other verbs and adjectives which also goes with the two fruits, 
the semantic representation should be able to form the induction that 'lingou' might be something that similar
to a fruit, therefore edible and could be an argument for 'eat'.

While the description above seems to be a specific type of analogy task, it is actually a paramount property
that give rise to the fruitful word semantics and language. First, although in the example, we use 'verb-noun' pair, it 
can be generalized to all word pairs under grammaticality, e.g. (verb-preposition phrase, adjective-noun, verb-adverb).
Second, the capability of the semantic models to form novice combinations (compositions) based on observed data, equipped
with the recursive syntax, leads to the creation of infinite concepts and therefore infinite plausible meanings basing on 
a finite lexicon. Notice that the creation of new concepts is not a free combination due to generative syntactic rules, 
but also under the semantic constraint provided by the semantic representations, therefore, the new concepts are not
only grammatical, but also 'meaningful'. (e.g. eat lingou).

Thus, the first goal of the project, is to figure out the representational power of semantic models in representing D1.
In other words, what type of semantic models, and to what extent can they first represent the 'coupling' observed in 
the corpus, and then form inference on possible couplings which are not observed. Such representational power of semantic
models has the benefits of:
1. Forming semantic classification for words
2. Giving the potential to equip the generative language with generative meanings (in other words, making the generative language meaningful,
otherwise, the resultant generative language is only a set of formal symbols)

### D2: Representation of real composed concepts

While D1 is about the combinatorial property of words, D2 is about the same property with constraint. For example, in D1
while 'drink an apple' is bad, both 'drink coffee' and 'drink wine' are fine. In D2, we talk about 'drink wine' and 
'drink coffee' under constraint. Suppose that the location of the 'drink' is a bar, than 'drink wine' make more sense
than 'drink coffee'. To spell it out, the phrase 'drink wine in the bar' make more sense than the phrase 'drink wine in 
the cafe'. Here we see that the argument of the predicate 'drink' does not only depend on the verb itself, but also the 
prepositional phrase 'in the X' modifying the verb. Therefore, we call it 'second order dependence', referring to the 
choice of coupling depend on more than one factor.

Compared to D1, what D2 brings is the representation of composed concepts and meanings. In D1, only lexical items and the 
relation between lexical items are investigated, such as 'drink coffee' concerns the semantic soundness or relation between
'drink' and 'coffee', while in D2, the items to be investigate are not only words, but also phrases, which are compositional
structures. In D2, we consider the semantic soundness of 'drink coffee in the bar', that is the relation among 'drink',
'coffee' and 'bar', and to be more precise, the relation between 'drink coffee' and 'bar'. In this case, models have to
come up with representations not only for the words, but also for the phrase 'drink coffee', which should be derived from
the representation of the two words consisting it, and this is the problem of 'representing compositional structure'.

In this scenario, there are approaches which leave aside compositionality, such as some type of baysian models, considering
the likelihood of pairing 'drink' and 'coffee' conditioning on the constraint of 'bar'. While this type of model might work
for this example, it take the cost of missing syntactic information, which could be paramount to representing semantics.
And if we take the syntactic structure seriously, it is not avoidable to deal with the problem of representing composed 
structures, like 'drink coffee', or 'chase cat'.

And similar to the first goal, the second goal of the project, is to figure out the representational power of semantic models
accounting for D2. In other words, what type of semantic models, and to what extent can they first represent the **'constrained
coupling'** observed in the corpus, and then form inference on possible couplings which are not observed. For example, in
the corpus, both 'coffee', and 'wine' couple with 'drink', while the former happens 'in the cafe', and the latter 'in the bar'
a semantic model need to:
1. say ok to both 'drink coffee' and 'drink wine', but take 'drink wine in the bar' while reject 'drink coffee in the bar'.
2. for 'drink tea', which is not specified in the corpus that it is drunk in a bar or a cafe, the model need to infer that
it is more likely to happen in a bar or a cafe, basing on how similar it is to 'drink coffee' and 'drink wine'.

What makes D2 a problem much more difficult than D1 is the issue of compositionality. We need to not only take care of
the words themselves, but also consider the composition of the words into a complex structure. Moreover, something need to
be mentioned here is that, the representation we are talking about here is not anything like Montague-ish formal semantic,
which only provide a mechanism to formally compose the meaning of 'drink coffee in the bar', while having no answer for
why that makes less sense than 'drink wine in the bar', or 'drink coffe in the cafe', or even more than that, how similar
is the notion of 'drink coffee in the bar' to the that of 'reading newspaper in the stadium'. We need kind of the representations
having a decent account for the compositionality, while being able to articulate the relations between these composed 
structures, like representing the relations of the words in D1.


## World

## Corpus

## Semantic Models

